{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Train SO-101 Robot with LeRobot\n",
        "\n",
        "This notebook trains an **ACT (Action Chunking Transformer)** policy on your demonstration data from RoboSim.\n",
        "\n",
        "## Prerequisites\n",
        "- Dataset uploaded to HuggingFace Hub from RoboSim\n",
        "- Google account (for Colab GPU access)\n",
        "\n",
        "## Instructions\n",
        "1. Enter your dataset ID below\n",
        "2. Click **Runtime ‚Üí Run all**\n",
        "3. Wait ~2 hours for training\n",
        "4. Download your trained model!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Configuration\n",
        "\n",
        "Enter your HuggingFace dataset ID and token below:"
      ],
      "metadata": {
        "id": "config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configuration { display-mode: \"form\" }\n",
        "#@markdown ### Your Dataset\n",
        "DATASET_REPO_ID = \"your-username/your-dataset-name\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### HuggingFace Token (for saving trained model)\n",
        "HF_TOKEN = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Training Settings\n",
        "TRAINING_STEPS = 50000 #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 8 #@param {type:\"integer\"}\n",
        "SAVE_MODEL_TO_HF = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "# Validate inputs\n",
        "if DATASET_REPO_ID == \"your-username/your-dataset-name\":\n",
        "    print(\"‚ö†Ô∏è  Please enter your actual dataset ID above!\")\n",
        "    print(\"   Example: hshadab/cube-pickup-training-1234567890\")\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset: {DATASET_REPO_ID}\")\n",
        "    print(f\"‚úÖ Training steps: {TRAINING_STEPS:,}\")\n",
        "    print(f\"‚úÖ Batch size: {BATCH_SIZE}\")\n",
        "    if SAVE_MODEL_TO_HF and HF_TOKEN:\n",
        "        print(f\"‚úÖ Will save trained model to HuggingFace\")\n",
        "    elif SAVE_MODEL_TO_HF:\n",
        "        print(\"‚ö†Ô∏è  Enter HF_TOKEN to save model to HuggingFace\")"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Check GPU & Install Dependencies"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check GPU availability\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"‚úÖ GPU: {gpu_name}\")\n",
        "    print(f\"‚úÖ Memory: {gpu_memory:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ùå No GPU detected!\")\n",
        "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install LeRobot (takes ~3 minutes)\n",
        "print(\"Installing LeRobot...\")\n",
        "!pip install -q lerobot\n",
        "\n",
        "# Verify installation\n",
        "import lerobot\n",
        "print(f\"‚úÖ LeRobot {lerobot.__version__} installed\")"
      ],
      "metadata": {
        "id": "install_lerobot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Load & Inspect Dataset"
      ],
      "metadata": {
        "id": "dataset_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load dataset from HuggingFace\n",
        "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
        "\n",
        "print(f\"Loading dataset: {DATASET_REPO_ID}\")\n",
        "dataset = LeRobotDataset(DATASET_REPO_ID)\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset loaded!\")\n",
        "print(f\"   Episodes: {dataset.num_episodes}\")\n",
        "print(f\"   Total frames: {len(dataset)}\")\n",
        "print(f\"   Features: {list(dataset.features.keys())}\")"
      ],
      "metadata": {
        "id": "load_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Preview a sample from the dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get first frame\n",
        "sample = dataset[0]\n",
        "\n",
        "print(\"Sample data:\")\n",
        "for key, value in sample.items():\n",
        "    if hasattr(value, 'shape'):\n",
        "        print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")"
      ],
      "metadata": {
        "id": "preview_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Train ACT Policy\n",
        "\n",
        "This will take **~2 hours** on a T4 GPU for 50,000 steps.\n",
        "\n",
        "You can monitor progress in the output below."
      ],
      "metadata": {
        "id": "training_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start training\n",
        "import os\n",
        "\n",
        "# Set HuggingFace token for model upload\n",
        "if HF_TOKEN:\n",
        "    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "\n",
        "# Create output directory\n",
        "OUTPUT_DIR = f\"outputs/train/{DATASET_REPO_ID.split('/')[-1]}\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üöÄ Starting training...\")\n",
        "print(f\"   Dataset: {DATASET_REPO_ID}\")\n",
        "print(f\"   Steps: {TRAINING_STEPS:,}\")\n",
        "print(f\"   Output: {OUTPUT_DIR}\")\n",
        "print(f\"\\nThis will take ~2 hours. Go grab a coffee! ‚òï\\n\")\n",
        "\n",
        "# Run training\n",
        "!python -m lerobot.scripts.train \\\n",
        "    policy=act \\\n",
        "    env=so100_real \\\n",
        "    dataset_repo_id={DATASET_REPO_ID} \\\n",
        "    training.num_workers=4 \\\n",
        "    training.batch_size={BATCH_SIZE} \\\n",
        "    training.steps={TRAINING_STEPS} \\\n",
        "    training.save_freq=10000 \\\n",
        "    training.log_freq=100 \\\n",
        "    eval.n_episodes=0 \\\n",
        "    wandb.enable=false \\\n",
        "    hydra.run.dir={OUTPUT_DIR}"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Save Trained Model"
      ],
      "metadata": {
        "id": "save_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload trained model to HuggingFace\n",
        "from huggingface_hub import HfApi, upload_folder\n",
        "import os\n",
        "\n",
        "if SAVE_MODEL_TO_HF and HF_TOKEN:\n",
        "    # Find the checkpoint directory\n",
        "    checkpoint_dir = f\"{OUTPUT_DIR}/checkpoints/last/pretrained_model\"\n",
        "    \n",
        "    if os.path.exists(checkpoint_dir):\n",
        "        # Create model repo name\n",
        "        dataset_name = DATASET_REPO_ID.split('/')[-1]\n",
        "        model_repo_id = f\"{DATASET_REPO_ID.split('/')[0]}/{dataset_name}-act-policy\"\n",
        "        \n",
        "        print(f\"Uploading model to: {model_repo_id}\")\n",
        "        \n",
        "        api = HfApi(token=HF_TOKEN)\n",
        "        \n",
        "        # Create repo if it doesn't exist\n",
        "        try:\n",
        "            api.create_repo(model_repo_id, exist_ok=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Note: {e}\")\n",
        "        \n",
        "        # Upload\n",
        "        upload_folder(\n",
        "            folder_path=checkpoint_dir,\n",
        "            repo_id=model_repo_id,\n",
        "            token=HF_TOKEN\n",
        "        )\n",
        "        \n",
        "        print(f\"\\n‚úÖ Model uploaded!\")\n",
        "        print(f\"   https://huggingface.co/{model_repo_id}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Checkpoint not found at {checkpoint_dir}\")\n",
        "        print(\"   Make sure training completed successfully.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Skipping upload (no HF_TOKEN or SAVE_MODEL_TO_HF=False)\")\n",
        "    print(f\"   Model saved locally at: {OUTPUT_DIR}/checkpoints/\")"
      ],
      "metadata": {
        "id": "upload_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download model to your computer\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "checkpoint_dir = f\"{OUTPUT_DIR}/checkpoints/last/pretrained_model\"\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    # Create zip file\n",
        "    zip_name = f\"{DATASET_REPO_ID.split('/')[-1]}_act_policy\"\n",
        "    shutil.make_archive(zip_name, 'zip', checkpoint_dir)\n",
        "    \n",
        "    print(f\"üì• Downloading {zip_name}.zip...\")\n",
        "    files.download(f\"{zip_name}.zip\")\n",
        "else:\n",
        "    print(f\"‚ùå No checkpoint found. Run training first.\")"
      ],
      "metadata": {
        "id": "download_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Deploy to Real SO-101\n",
        "\n",
        "Once you have the trained model, run this on your computer with the SO-101 connected:\n",
        "\n",
        "```bash\n",
        "# Install LeRobot on your computer\n",
        "pip install lerobot\n",
        "\n",
        "# Run the trained policy\n",
        "python -m lerobot.scripts.control_robot record \\\n",
        "    --robot-path lerobot/configs/robot/so100.yaml \\\n",
        "    --policy-path PATH_TO_YOUR_MODEL \\\n",
        "    --fps 30\n",
        "```\n",
        "\n",
        "Or if you uploaded to HuggingFace:\n",
        "\n",
        "```bash\n",
        "python -m lerobot.scripts.control_robot record \\\n",
        "    --robot-path lerobot/configs/robot/so100.yaml \\\n",
        "    --policy.path=YOUR_USERNAME/YOUR_MODEL_REPO \\\n",
        "    --fps 30\n",
        "```"
      ],
      "metadata": {
        "id": "deploy_instructions"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üéâ Done!\n",
        "\n",
        "You've successfully trained an ACT policy for your SO-101 robot.\n",
        "\n",
        "**Next steps:**\n",
        "1. Download or use the HuggingFace model\n",
        "2. Connect your real SO-101 robot\n",
        "3. Run the policy and watch it pick up objects!\n",
        "\n",
        "**Need help?** Check out:\n",
        "- [LeRobot Documentation](https://github.com/huggingface/lerobot)\n",
        "- [RoboSim GitHub](https://github.com/YOUR_REPO)\n",
        "\n",
        "---\n",
        "*Generated by RoboSim - Train robots with AI*"
      ],
      "metadata": {
        "id": "done"
      }
    }
  ]
}
